{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with different website layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /home/kate/.local/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests) (1.26.9)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.3.2 requests-2.32.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE: \n",
      "            Robotic rulemaking\n",
      "          \n",
      "URL: https://www.brookings.edu/research/robotic-rulemaking/\n",
      "BODY:\n",
      " No content found\n",
      "TITLE: \n",
      "      Dogecoin jumps after Elon Musk replaces Twitter bird with Shiba Inu\n",
      "    \n",
      "URL: https://www.cnn.com/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html\n",
      "BODY:\n",
      " \n",
      "\n",
      "\n",
      "New York\n",
      "CNN\n",
      "         — \n",
      "    \n",
      "\n",
      "\n",
      "            Twitter’s traditional bird icon was booted and replaced with an image of a Shiba Inu, an apparent nod to dogecoin, the joke cryptocurrency that CEO Elon Musk is being sued over. \n",
      "    \n",
      "\n",
      "            Musk addressed the change Monday afternoon, tweeting, “as promised” above an image of a year-old conversation in which another user suggested that Musk “just buy Twitter” and “change the bird logo to a doge.” \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CNN/Adobe Stock\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Elon Musk's Twitter promised a purge of blue check marks. Instead he singled out one account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            The doge logo appeared on the site two days after Musk asked a judge to throw out a $258 billion racketeering lawsuit accusing him of running a pyramid scheme to support the dogecoin, according to Reuters.\n",
      "\n",
      "\n",
      "            Lawyers for Musk and Tesla called the lawsuit by dogecoin investors a “fanciful work of fiction” over Musk’s “innocuous and often silly tweets.”\n",
      "    \n",
      "\n",
      "            It wasn’t clear whether the logo change was permanent. Musk has been known to use Twitter to troll both his fans and his critics. \n",
      "    \n",
      "\n",
      "            The price of dogecoin, which is typically volatile, was up more than 20% over the past 24 hours, to about 9 cents. It was trading just under 8 cents Monday morning.\n",
      "    \n",
      "\n",
      "Dogecoin was created December 6, 2013, by a pair of software engineers — as a joke. The name is a nod to the “doge” meme that became popular a decade ago. Its Shiba Inu mascot mimicks that meme: a dog surrounded by a bunch of Comic Sans text in broken English.\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "\n",
    "class Content:\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "    \n",
    "    def print(self):\n",
    "        print(f'TITLE: {self.title}')\n",
    "        print(f'URL: {self.url}')\n",
    "        print(f'BODY:\\n {self.body}')\n",
    "\n",
    "def scrapeCNN(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        bs = BeautifulSoup(response.content, 'html.parser')\n",
    "        title_element = bs.find('h1')\n",
    "        body_element = bs.find('div', {'class': 'article__content'})\n",
    "        \n",
    "        title = title_element.text if title_element else 'No title found'\n",
    "        body = body_element.text if body_element else 'No content found'\n",
    "        \n",
    "        return Content(url, title, body)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve CNN article. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def scrapeBrookings(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        bs = BeautifulSoup(response.content, 'html.parser')\n",
    "        title_element = bs.find('h1')\n",
    "        body_element = bs.find('div', {'class': 'post-body'})\n",
    "        \n",
    "        title = title_element.text if title_element else 'No title found'\n",
    "        body = body_element.text if body_element else 'No content found'\n",
    "        \n",
    "        return Content(url, title, body)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Brookings article. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "url = 'https://www.brookings.edu/research/robotic-rulemaking/'\n",
    "content = scrapeBrookings(url)\n",
    "if content:\n",
    "    content.print()\n",
    "\n",
    "url = 'https://www.cnn.com/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html'\n",
    "content = scrapeCNN(url)\n",
    "if content:\n",
    "    content.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"\n",
    "    Common base class for all articles/pages\n",
    "    \"\"\"\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        Flexible printing function controls output\n",
    "        \"\"\"\n",
    "        print(f'URL: {self.url}')\n",
    "        print(f'TITLE: {self.title}')\n",
    "        print(f'BODY:\\n{self.body}')\n",
    "\n",
    "class Website:\n",
    "    \"\"\" \n",
    "    Contains information about website structure\n",
    "    \"\"\"\n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def getPage(url):\n",
    "        try:\n",
    "            html = urlopen(url)\n",
    "        except Exception:\n",
    "            return None\n",
    "        return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    def safeGet(bs, selector):\n",
    "        \"\"\"\n",
    "        Utilty function used to get a content string from a Beautiful Soup\n",
    "        object and a selector. Returns an empty string if no object\n",
    "        is found for the given selector\n",
    "        \"\"\"\n",
    "        selectedElems = bs.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "\n",
    "    def getContent(website, path):\n",
    "        \"\"\"\n",
    "        Extract content from a given page URL\n",
    "        \"\"\"\n",
    "        url = website.url+path\n",
    "        bs = Crawler.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = Crawler.safeGet(bs, website.titleTag)\n",
    "            body = Crawler.safeGet(bs, website.bodyTag)\n",
    "            return Content(url, title, body)\n",
    "        return Content(url, '', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://www.oreilly.com/library/view/web-scraping-with/9781491910283\n",
      "TITLE: Web Scraping with Python\n",
      "BODY:\n",
      "\n",
      "\n",
      "\n",
      "      \n",
      "        Book\n",
      "      description\n",
      "Learn web scraping and crawling techniques to access unlimited data from any web source in any format. With this practical guide, you’ll learn how to use Python scripts and web APIs to gather and process data from thousands—or even millions—of web pages at once.Ideal for programmers, security professionals, and web administrators familiar with Python, this book not only teaches basic web scraping mechanics, but also delves into more advanced topics, such as analyzing raw data or using scrapers for frontend website testing. Code samples are available to help you understand the concepts in practice.\n",
      "Show and hide more\n",
      "\n",
      "Publisher resources\n",
      "View/Submit Errata\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "URL: https://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0\n",
      "TITLE: \n",
      "BODY:\n",
      "\n",
      "URL: https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/\n",
      "TITLE: \n",
      "BODY:\n",
      "\n",
      "URL: https://www.cnn.com/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html\n",
      "TITLE: \n",
      "      Dogecoin jumps after Elon Musk replaces Twitter bird with Shiba Inu\n",
      "    \n",
      "BODY:\n",
      "\n",
      "\n",
      "\n",
      "New York\n",
      "CNN\n",
      "         — \n",
      "    \n",
      "\n",
      "\n",
      "            Twitter’s traditional bird icon was booted and replaced with an image of a Shiba Inu, an apparent nod to dogecoin, the joke cryptocurrency that CEO Elon Musk is being sued over. \n",
      "    \n",
      "\n",
      "            Musk addressed the change Monday afternoon, tweeting, “as promised” above an image of a year-old conversation in which another user suggested that Musk “just buy Twitter” and “change the bird logo to a doge.” \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CNN/Adobe Stock\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Elon Musk's Twitter promised a purge of blue check marks. Instead he singled out one account\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "            The doge logo appeared on the site two days after Musk asked a judge to throw out a $258 billion racketeering lawsuit accusing him of running a pyramid scheme to support the dogecoin, according to Reuters.\n",
      "\n",
      "\n",
      "            Lawyers for Musk and Tesla called the lawsuit by dogecoin investors a “fanciful work of fiction” over Musk’s “innocuous and often silly tweets.”\n",
      "    \n",
      "\n",
      "            It wasn’t clear whether the logo change was permanent. Musk has been known to use Twitter to troll both his fans and his critics. \n",
      "    \n",
      "\n",
      "            The price of dogecoin, which is typically volatile, was up more than 20% over the past 24 hours, to about 9 cents. It was trading just under 8 cents Monday morning.\n",
      "    \n",
      "\n",
      "Dogecoin was created December 6, 2013, by a pair of software engineers — as a joke. The name is a nod to the “doge” meme that became popular a decade ago. Its Shiba Inu mascot mimicks that meme: a dog surrounded by a bunch of Comic Sans text in broken English.\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "siteData = [\n",
    "    ['O\\'Reilly Media', 'https://www.oreilly.com', 'h1', 'div.title-description'],\n",
    "    ['Reuters', 'https://www.reuters.com', 'h1', 'div.ArticleBodyWrapper'],\n",
    "    ['Brookings', 'https://www.brookings.edu', 'h1', 'div.post-body'],\n",
    "    ['CNN', 'https://www.cnn.com', 'h1', 'div.article__content']\n",
    "]\n",
    "websites = []\n",
    "for name, url, title, body in siteData:\n",
    "    websites.append(Website(name, url, title, body))\n",
    "\n",
    "Crawler.getContent(websites[0], '/library/view/web-scraping-with/9781491910283').print()\n",
    "Crawler.getContent(\n",
    "    websites[1], '/article/us-usa-epa-pruitt-idUSKBN19W2D0').print()\n",
    "Crawler.getContent(\n",
    "    websites[2],\n",
    "    '/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/').print()\n",
    "Crawler.getContent(\n",
    "    websites[3], \n",
    "    '/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html').print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling through sites with search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"Common base class for all articles/pages\"\"\"\n",
    "\n",
    "    def __init__(self, topic, url, title, body):\n",
    "        self.topic = topic\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        self.url = url\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        Flexible printing function controls output\n",
    "        \"\"\"\n",
    "        print(f'New article found for topic: {self.topic}')\n",
    "        print(f'URL: {self.url}')\n",
    "        print(f'TITLE: {self.title}')\n",
    "        print(f'BODY:\\n{self.body}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    \"\"\"Contains information about website structure\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.searchUrl = searchUrl\n",
    "        self.resultListing = resultListing\n",
    "        self.resultUrl = resultUrl\n",
    "        self.absoluteUrl = absoluteUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page is none for topic: 'python'\n",
      "page is none for topic: 'python'\n",
      "page is none for topic: 'data%20science'\n",
      "page is none for topic: 'data%20science'\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, website):\n",
    "        self.site = website\n",
    "        self.found = {}\n",
    "\n",
    "    def getPage(url):\n",
    "        try:\n",
    "            html = urlopen(url)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "        return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    def safeGet(bs, selector):\n",
    "        \"\"\"\n",
    "        Utilty function used to get a content string from a Beautiful Soup\n",
    "        object and a selector. Returns an empty string if no object\n",
    "        is found for the given selector\n",
    "        \"\"\"\n",
    "        selectedElems = bs.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "\n",
    "    def getContent(self, topic, url):\n",
    "        \"\"\"\n",
    "        Extract content from a given page URL\n",
    "        \"\"\"\n",
    "        bs = Crawler.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = Crawler.safeGet(bs, self.site.titleTag)\n",
    "            body = Crawler.safeGet(bs, self.site.bodyTag)\n",
    "            return Content(topic, url, title, body)\n",
    "        return Content(topic, url, '', '')\n",
    "\n",
    "    def search(self, topic):\n",
    "        \"\"\"\n",
    "        Searches a given website for a given topic and records all pages found\n",
    "        \"\"\"\n",
    "        bs = Crawler.getPage(self.site.searchUrl + topic)\n",
    "        if bs is not None:\n",
    "            searchResults = bs.select(self.site.resultListing)\n",
    "            for result in searchResults:\n",
    "                url = result.select(self.site.resultUrl)[0].attrs['href']\n",
    "                # Check to see whether it's a relative or an absolute URL\n",
    "                url = url if self.site.absoluteUrl else self.site.url + url\n",
    "                if url not in self.found:\n",
    "                    self.found[url] = self.getContent(topic, url)\n",
    "                self.found[url].print()\n",
    "        else:\n",
    "            print(f'page is none for topic: {topic!r}')\n",
    "\n",
    "\n",
    "\n",
    "siteData = [\n",
    "    ['Reuters', 'http://reuters.com', 'https://www.reuters.com/search/news?blob=', 'div.search-result-indiv',\n",
    "        'h3.search-result-title a', False, 'h1', 'div.ArticleBodyWrapper'],\n",
    "    ['Brookings', 'http://www.brookings.edu', 'https://www.brookings.edu/search/?s=',\n",
    "        'div.article-info', 'h4.title a', True, 'h1', 'div.core-block']\n",
    "]\n",
    "sites = []\n",
    "for name, url, search, rListing, rUrl, absUrl, tt, bt in siteData:\n",
    "    sites.append(Website(name, url, search, rListing, rUrl, absUrl, tt, bt))\n",
    "\n",
    "crawlers = [Crawler(site) for site in sites]\n",
    "topics = ['python', 'data%20science']\n",
    "\n",
    "for topic in topics:\n",
    "    for crawler in crawlers:\n",
    "        crawler.search(topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling Sites through Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "\n",
    "    def __init__(self, name, url, targetPattern, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.targetPattern = targetPattern\n",
    "        self.absoluteUrl = absoluteUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "\n",
    "\n",
    "class Content:\n",
    "\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "\n",
    "    def print(self):\n",
    "        print(f'URL: {self.url}')\n",
    "        print(f'TITLE: {self.title}')\n",
    "        print(f'BODY:\\n{self.body}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not get page https://brookings.edu HTTP Error 403: Forbidden\n",
      "Crawling page https://brookings.edu returned None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self, site):\n",
    "        self.site = site\n",
    "        self.visited = {}\n",
    "\n",
    "    def getPage(url):\n",
    "        try:\n",
    "            html = urlopen(url)\n",
    "        except Exception as e:\n",
    "            print('Could not get page', url, e)\n",
    "            return None\n",
    "        return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    def safeGet(bs, selector):\n",
    "        selectedElems = bs.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "\n",
    "    def getContent(self, url):\n",
    "        \"\"\"\n",
    "        Extract content from a given page URL\n",
    "        \"\"\"\n",
    "        bs = Crawler.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = Crawler.safeGet(bs, self.site.titleTag)\n",
    "            body = Crawler.safeGet(bs, self.site.bodyTag)\n",
    "            return Content(url, title, body)\n",
    "        return Content(url, '', '')\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"\n",
    "        Get pages from website home page\n",
    "        \"\"\"\n",
    "        bs = Crawler.getPage(self.site.url)\n",
    "        if bs is not None:\n",
    "            targetPages = bs.findAll('a', href=re.compile(self.site.targetPattern))\n",
    "            for targetPage in targetPages:\n",
    "                url = targetPage.attrs['href']\n",
    "                url = url if self.site.absoluteUrl else f'{self.site.url}{targetPage}'\n",
    "                if url not in self.visited:\n",
    "                    self.visited[url] = self.getContent(url)\n",
    "                    self.visited[url].print()\n",
    "        else:\n",
    "            print(f'Crawling page {self.site.url} returned None')\n",
    "\n",
    "\n",
    "brookings = Website('Reuters', 'https://brookings.edu', '\\/(research|blog)\\/', True, 'h1', 'div.post-body')\n",
    "crawler = Crawler(brookings)\n",
    "crawler.crawl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling multiple page types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    \"\"\"Common base class for all articles/pages\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product(Website):\n",
    "    \"\"\"Contains information for scraping a product page\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, productNumber, price):\n",
    "        Website.__init__(self, name, url, TitleTag)\n",
    "        self.productNumberTag = productNumberTag\n",
    "        self.priceTag = priceTag\n",
    "\n",
    "class Article(Website):\n",
    "    \"\"\"Contains information for scraping an article page\"\"\"\n",
    "\n",
    "    def __init__(self, name, url, titleTag, bodyTag, dateTag):\n",
    "        Website.__init__(self, name, url, titleTag)\n",
    "        self.bodyTag = bodyTag\n",
    "        self.dateTag = dateTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
